{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is an exercise in the [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/alexisbcook/xgboost).**\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"In this exercise, you will use your new knowledge to train a model with **gradient boosting**.\n\n# Setup\n\nThe questions below will give you feedback on your work. Run the following cell to set up the feedback system.","metadata":{}},{"cell_type":"code","source":"# Set up code checking\nimport os\nif not os.path.exists(\"../input/train.csv\"):\n    os.symlink(\"../input/home-data-for-ml-course/train.csv\", \"../input/train.csv\")  \n    os.symlink(\"../input/home-data-for-ml-course/test.csv\", \"../input/test.csv\") \nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.ml_intermediate.ex6 import *\nprint(\"Setup Complete\")","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:22:28.587973Z","iopub.execute_input":"2023-07-25T21:22:28.588470Z","iopub.status.idle":"2023-07-25T21:22:30.668843Z","shell.execute_reply.started":"2023-07-25T21:22:28.588434Z","shell.execute_reply":"2023-07-25T21:22:30.667663Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Setup Complete\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You will work with the [Housing Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course) dataset from the previous exercise. \n\n![Ames Housing dataset image](https://storage.googleapis.com/kaggle-media/learn/images/lTJVG4e.png)\n\nRun the next code cell without changes to load the training and validation sets in `X_train`, `X_valid`, `y_train`, and `y_valid`.  The test set is loaded in `X_test`.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX = pd.read_csv('../input/train.csv', index_col='Id')\nX_test_full = pd.read_csv('../input/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice              \nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n# 选择基数相对较低的分类列\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n'''\n大多数机器学习模型都不能直接处理分类变量，需要将它们转换为数值形式。常见的转换方法是独热编码（One-Hot Encoding），即为每个唯一的类别创建一个新的二进制特征。如果一个分类变量的基数非常高，那么独热编码会导致数据集的维度大大增加，这可能会导致模型训练时间过长，需要更多的计算资源，甚至可能导致模型过拟合。\n选择基数相对较低的分类列是减少分类特征中唯一类别数量的一种方法，有助于避免one-hot编码问题，提高机器学习模型的性能\n然而，值得注意的是，选择低基数列并不总是最好的方法，它取决于具体的数据集和当前的问题。在某些情况下，高基数分类特征可能包含可以提高模型准确性的重要信息，可能需要使用更先进的技术来处理它们\n选择基数较低的分类列是一个实用的策略，可以帮助我们保持数据集的维度在一个可管理的范围内，同时也可以避免一些与高基数分类变量相关的问题。然而，这也可能会导致一些信息的丢失，因为我们可能会忽略一些可能对目标变量有影响的分类变量。在实际应用中，如何处理高基数分类变量是一个需要权衡的问题，可能需要根据具体的数据和任务来决定。\n'''\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)\n\n'''\n首先，pd.get_dummies()函数用于将分类变量转换为独热编码形式。\n例如，如果我们有一个名为\"颜色\"的列，其中包含\"红色\"、\"蓝色\"和\"绿色\"三个值，\n那么pd.get_dummies()会创建三个新的列，分别名为\"颜色_红色\"、\"颜色_蓝色\"和\"颜色_绿色\"。如果原始列中的值是\"红色\"，那么\"颜色_红色\"列的值就是1，其他两列的值就是0。\n\n然后，align()函数用于确保训练集和验证集（或测试集）有相同的列。\n这是必要的，因为如果某个分类变量在训练集中出现的值在验证集（或测试集）中没有出现，\n那么pd.get_dummies()会在训练集和验证集（或测试集）中创建不同的列。\nalign()函数通过添加缺失的列（并用0填充）和删除额外的列来解决这个问题。\njoin='left'参数表示我们要保留左边数据集（即训练集）的列，\naxis=1表示我们要沿着列的方向（即横向）对齐数据集。\n在align()函数中，axis=1表示我们要沿着列的方向对齐数据集，即确保所有数据集有相同的列。如果我们设置axis=0，那么我们将沿着行的方向对齐数据集，即确保所有数据集有相同的行。然而，在大多数情况下，我们更关心的是列的对齐（因为列通常代表特征），所以axis=1用得更多。\n'''","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:22:30.673934Z","iopub.execute_input":"2023-07-25T21:22:30.676690Z","iopub.status.idle":"2023-07-25T21:22:30.896511Z","shell.execute_reply.started":"2023-07-25T21:22:30.676650Z","shell.execute_reply":"2023-07-25T21:22:30.895249Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'\\n首先，pd.get_dummies()函数用于将分类变量转换为独热编码形式。\\n例如，如果我们有一个名为\"颜色\"的列，其中包含\"红色\"、\"蓝色\"和\"绿色\"三个值，\\n那么pd.get_dummies()会创建三个新的列，分别名为\"颜色_红色\"、\"颜色_蓝色\"和\"颜色_绿色\"。如果原始列中的值是\"红色\"，那么\"颜色_红色\"列的值就是1，其他两列的值就是0。\\n\\n然后，align()函数用于确保训练集和验证集（或测试集）有相同的列。\\n这是必要的，因为如果某个分类变量在训练集中出现的值在验证集（或测试集）中没有出现，\\n那么pd.get_dummies()会在训练集和验证集（或测试集）中创建不同的列。\\nalign()函数通过添加缺失的列（并用0填充）和删除额外的列来解决这个问题。\\njoin=\\'left\\'参数表示我们要保留左边数据集（即训练集）的列，\\naxis=1表示我们要沿着列的方向（即横向）对齐数据集。\\n在align()函数中，axis=1表示我们要沿着列的方向对齐数据集，即确保所有数据集有相同的列。如果我们设置axis=0，那么我们将沿着行的方向对齐数据集，即确保所有数据集有相同的行。然而，在大多数情况下，我们更关心的是列的对齐（因为列通常代表特征），所以axis=1用得更多。\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Step 1: Build model\n\n### Part A\n\nIn this step, you'll build and train your first model with gradient boosting.\n\n- Begin by setting `my_model_1` to an XGBoost model.  Use the [XGBRegressor](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor) class, and set the random seed to 0 (`random_state=0`).  **Leave all other parameters as default.**\n- Then, fit the model to the training data in `X_train` and `y_train`.","metadata":{}},{"cell_type":"markdown","source":"# 步骤 1：建立模型\n## A 部分\n在本步骤中，您将使用梯度提升技术建立并训练第一个模型。\n\n- 首先将 my_model_1 设置为 XGBoost 模型。\n- 使用 XGBRegressor 类，并将随机种子设为 0（random_state=0）。其他参数保持默认值。\n- 然后，根据 X_train 和 y_train 中的训练数据拟合模型。","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\n# Define the model\nmy_model_1 = XGBRegressor(random_state=0)\n\n# Fit the model\nmy_model_1.fit(X_train, y_train)\n\n# Check your answer\nstep_1.a.check()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:22:30.898347Z","iopub.execute_input":"2023-07-25T21:22:30.898812Z","iopub.status.idle":"2023-07-25T21:22:31.740131Z","shell.execute_reply.started":"2023-07-25T21:22:30.898772Z","shell.execute_reply":"2023-07-25T21:22:31.739007Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"1.1_Model1A\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#step_1.a.hint()\n#step_1.a.solution()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:22:31.743258Z","iopub.execute_input":"2023-07-25T21:22:31.744010Z","iopub.status.idle":"2023-07-25T21:22:31.748652Z","shell.execute_reply.started":"2023-07-25T21:22:31.743969Z","shell.execute_reply":"2023-07-25T21:22:31.747709Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Part B\n\nSet `predictions_1` to the model's predictions for the validation data.  Recall that the validation features are stored in `X_valid`.","metadata":{}},{"cell_type":"code","source":"\n# Get predictions\npredictions_1 =  my_model_1.predict(X_valid)\n\n# Check your answer\nstep_1.b.check()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:22:31.749840Z","iopub.execute_input":"2023-07-25T21:22:31.753601Z","iopub.status.idle":"2023-07-25T21:22:31.787842Z","shell.execute_reply.started":"2023-07-25T21:22:31.753561Z","shell.execute_reply":"2023-07-25T21:22:31.786904Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"1.2_Model1B\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#step_1.b.hint()\n#step_1.b.solution()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:22:31.789470Z","iopub.execute_input":"2023-07-25T21:22:31.790132Z","iopub.status.idle":"2023-07-25T21:22:31.795255Z","shell.execute_reply.started":"2023-07-25T21:22:31.790087Z","shell.execute_reply":"2023-07-25T21:22:31.794368Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Part C\n\nFinally, use the `mean_absolute_error()` function to calculate the mean absolute error (MAE) corresponding to the predictions for the validation set.  Recall that the labels for the validation data are stored in `y_valid`.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\n# Calculate MAE\nmae_1 = mean_absolute_error(predictions_1, y_valid)\n\n# Uncomment to print MAE\nprint(\"Mean Absolute Error:\" , mae_1)\n\n# Check your answer\nstep_1.c.check()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:22:31.796949Z","iopub.execute_input":"2023-07-25T21:22:31.797286Z","iopub.status.idle":"2023-07-25T21:22:31.814626Z","shell.execute_reply.started":"2023-07-25T21:22:31.797256Z","shell.execute_reply":"2023-07-25T21:22:31.813501Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Mean Absolute Error: 17662.736729452055\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"1.3_Model1C\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#step_1.c.hint()\n#step_1.c.solution()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:22:31.816132Z","iopub.execute_input":"2023-07-25T21:22:31.816522Z","iopub.status.idle":"2023-07-25T21:22:31.825112Z","shell.execute_reply.started":"2023-07-25T21:22:31.816493Z","shell.execute_reply":"2023-07-25T21:22:31.823587Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Improve the model\n\nNow that you've trained a default model as baseline, it's time to tinker with the parameters, to see if you can get better performance!\n- Begin by setting `my_model_2` to an XGBoost model, using the [XGBRegressor](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor) class.  Use what you learned in the previous tutorial to figure out how to change the default parameters (like `n_estimators` and `learning_rate`) to get better results.\n- Then, fit the model to the training data in `X_train` and `y_train`.\n- Set `predictions_2` to the model's predictions for the validation data.  Recall that the validation features are stored in `X_valid`.\n- Finally, use the `mean_absolute_error()` function to calculate the mean absolute error (MAE) corresponding to the predictions on the validation set.  Recall that the labels for the validation data are stored in `y_valid`.\n\nIn order for this step to be marked correct, your model in `my_model_2` must attain lower MAE than the model in `my_model_1`. ","metadata":{}},{"cell_type":"markdown","source":"# 步骤 2：改进模型\n既然已经训练了一个默认模型作为基线，那么现在就该调整一下参数，看看能否获得更好的性能！\n\n- 首先，使用 XGBRegressor 类将 my_model_2 设置为 XGBoost 模型。利用在上一教程中学到的知识，找出如何更改默认参数（如 n_estimators 和 learning_rate）以获得更好的结果。\n- 然后，根据 X_train 和 y_train 中的训练数据拟合模型。\n    - 将 predictions_2 设置为模型对验证数据的预测。回顾一下，验证特征存储在 X_valid 中。\n- 最后，使用 mean_absolute_error() 函数计算与验证集预测相对应的平均绝对误差（MAE）。请注意，验证数据的标签存储在 y_valid 中。\n为了使这一步标记为正确，my_model_2 中的模型必须比 my_model_1 中的模型获得更低的 MAE。","metadata":{}},{"cell_type":"code","source":"# Define the model\nmy_model_2 = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n\n# Fit the model\nmy_model_2.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)],\n             verbose=False)\n\n# Get predictions\npredictions_2 = my_model_2.predict(X_valid)\n\n# Calculate MAE\nmae_2 = mean_absolute_error(predictions_2, y_valid)\n\n# Uncomment to print MAE\nprint(\"Mean Absolute Error:\" , mae_2)\n\n# Check your answer\nstep_2.check()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:22:31.826516Z","iopub.execute_input":"2023-07-25T21:22:31.826858Z","iopub.status.idle":"2023-07-25T21:22:34.103369Z","shell.execute_reply.started":"2023-07-25T21:22:31.826829Z","shell.execute_reply":"2023-07-25T21:22:34.102403Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Mean Absolute Error: 16802.965325342466\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"2_Model2\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#step_2.hint()\n#step_2.solution()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:22:34.108521Z","iopub.execute_input":"2023-07-25T21:22:34.111327Z","iopub.status.idle":"2023-07-25T21:22:34.116266Z","shell.execute_reply.started":"2023-07-25T21:22:34.111260Z","shell.execute_reply":"2023-07-25T21:22:34.114728Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Break the model\n\nIn this step, you will create a model that performs worse than the original model in Step 1.  This will help you to develop your intuition for how to set parameters.  You might even find that you accidentally get better performance, which is ultimately a nice problem to have and a valuable learning experience!\n- Begin by setting `my_model_3` to an XGBoost model, using the [XGBRegressor](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor) class.  Use what you learned in the previous tutorial to figure out how to change the default parameters (like `n_estimators` and `learning_rate`) to design a model to get high MAE.\n- Then, fit the model to the training data in `X_train` and `y_train`.\n- Set `predictions_3` to the model's predictions for the validation data.  Recall that the validation features are stored in `X_valid`.\n- Finally, use the `mean_absolute_error()` function to calculate the mean absolute error (MAE) corresponding to the predictions on the validation set.  Recall that the labels for the validation data are stored in `y_valid`.\n\nIn order for this step to be marked correct, your model in `my_model_3` must attain higher MAE than the model in `my_model_1`. ","metadata":{}},{"cell_type":"markdown","source":"# 步骤 3：打破模型\n在这一步中，您将创建一个性能比步骤 1 中的原始模型更差的模型。这将有助于你培养如何设置参数的直觉。您甚至可能会发现，您意外地获得了更好的性能，这最终是一个很好的问题，也是一个宝贵的学习经验！\n\n- 首先，使用 XGBRegressor 类将 my_model_3 设置为 XGBoost 模型。利用在上一教程中学到的知识，找出如何更改默认参数（如 n_estimators 和 learning_rate）来设计模型，以获得较高的 MAE。\n- 然后，将模型拟合到 X_train 和 y_train 中的训练数据。\n- 将 predictions_3 设置为模型对验证数据的预测值。回顾一下，验证特征存储在 X_valid 中。\n- 最后，使用 mean_absolute_error() 函数计算与验证集预测相对应的平均绝对误差（MAE）。请注意，验证数据的标签存储在 y_valid 中。\n为了使这一步标记为正确，my_model_3 中的模型必须比 my_model_1 中的模型获得更高的 MAE。","metadata":{}},{"cell_type":"code","source":"# Define the model\nmy_model_3 = XGBRegressor(n_estimators=10, learning_rate=0.01, n_jobs=4)\n# n_estimators=10, learning_rate=0.01 Mean Absolute Error: 164333.40517310574\n# n_estimators=10, learning_rate=0.05 mae: 109972.16702563142\n# n_estimators=10, learning_rate=0.1 mae: 65859.73539169521\n# n_estimators=10, learning_rate=0.5 mae: 20568.527838720034\n# n_estimators=10, learning_rate=1  mae: 25948.06064051798\n\n# n_estimators=100, learning_rate=0.01 Mean Absolute Error: 68584.79109589041\n# n_estimators=100, learning_rate=0.05, Mean Absolute Error: 17213.82253317637\n# n_estimators=100, learning_rate=0.1 Mean Absolute Error: 17388.557055329624\n# n_estimators=100, learning_rate=0.5 Mean Absolute Error: 20930.964656464042\n# n_estimators=100, learning_rate=1 Mean Absolute Error: 27386.556613869863\n\n# n_estimators=1000, learning_rate=0.01 Mean Absolute Error: 16918.452937714042\n# n_estimators=1000, learning_rate=0.05 Mean Absolute Error: 16688.691513270547\n# n_estimators=1000, learning_rate=0.1 Mean Absolute Error: 17308.831001177226\n# n_estimators=1000, learning_rate=0.5,Mean Absolute Error: 20928.494541952055\n# n_estimators=1000, learning_rate=1,Mean Absolute Error: 27386.61764233733\n\n# n_estimators=10000, learning_rate=0.01 Mean Absolute Error: 16836.033872003423\n# n_estimators=10000, learning_rate=0.05 Mean Absolute Error: 16681.90836365582\n# n_estimators=10000, learning_rate=0.1 Mean Absolute Error: 17308.523932470034\n# n_estimators=10000, learning_rate=0.5 Mean Absolute Error: 20928.012949486303\n# n_estimators=10000, learning_rate=1 Mean Absolute Error: 27386.61764233733\n\n# Fit the model\nmy_model_3.fit(X_train, y_train)\n\n# Get predictions\npredictions_3 = my_model_3.predict(X_valid)\n\n# Calculate MAE\nmae_3 = mean_absolute_error(predictions_3, y_valid)\n\n# Uncomment to print MAE\nprint(\"Mean Absolute Error:\" , mae_3)\n\n# Check your answer\nstep_3.check()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:22:34.118092Z","iopub.execute_input":"2023-07-25T21:22:34.118649Z","iopub.status.idle":"2023-07-25T21:22:34.264904Z","shell.execute_reply.started":"2023-07-25T21:22:34.118608Z","shell.execute_reply":"2023-07-25T21:22:34.263965Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Mean Absolute Error: 164333.40517310574\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"3_Model3\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#step_3.hint()\n#step_3.solution()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:22:34.268780Z","iopub.execute_input":"2023-07-25T21:22:34.271389Z","iopub.status.idle":"2023-07-25T21:22:34.276427Z","shell.execute_reply.started":"2023-07-25T21:22:34.271354Z","shell.execute_reply":"2023-07-25T21:22:34.275323Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# 创建一个字典，其中键是(n_estimators, learning_rate)对，值是对应的MAE\nresults = {\n    (10, 0.01): 164333.40517310574,\n    (10, 0.05): 109972.16702563142,\n    (10, 0.1): 65859.73539169521,\n    (10, 0.5): 20568.527838720034,\n    (10, 1): 25948.06064051798,\n    (100, 0.01): 68584.79109589041,\n    (100, 0.05): 17213.82253317637,\n    (100, 0.1): 17388.557055329624,\n    (100, 0.5): 20930.964656464042,\n    (100, 1): 27386.556613869863,\n    (1000, 0.01): 16918.452937714042,\n    (1000, 0.05): 16688.691513270547,\n    (1000, 0.1): 17308.831001177226,\n    (1000, 0.5): 20928.494541952055,\n    (1000, 1): 27386.61764233733,\n    (10000, 0.01): 16836.033872003423,\n    (10000, 0.05): 16681.90836365582,\n    (10000, 0.1): 17308.523932470034,\n    (10000, 0.5): 20928.012949486303,\n    (10000, 1): 27386.61764233733,\n}\n\n# 创建一个列表，其中包含所有的n_estimators和learning_rate值\nn_estimators_values = sorted(set(key[0] for key in results.keys()))\nlearning_rate_values = sorted(set(key[1] for key in results.keys()))\n\n# 创建一个二维数组，其中包含每个(n_estimators, learning_rate)对应的MAE\nmae_values = np.array([[results.get((n_estimators, learning_rate), np.nan) for learning_rate in learning_rate_values] for n_estimators in n_estimators_values])\n\n# 使用matplotlib创建一个热图\nplt.figure(figsize=(10, 8))\nplt.imshow(mae_values, cmap='hot', interpolation='nearest')\nplt.colorbar(label='MAE')\nplt.xticks(np.arange(len(learning_rate_values)), learning_rate_values)\nplt.yticks(np.arange(len(n_estimators_values)), n_estimators_values)\nplt.xlabel('Learning Rate')\nplt.ylabel('Number of Estimators')\nplt.title('MAE for different combinations of n_estimators and learning_rate')\nplt.show()\n\n# 使用matplotlib创建一个热图，这次我们使用'viridis'颜色映射，它提供了更好的颜色对比度\nplt.figure(figsize=(10, 8))\nplt.imshow(mae_values, cmap='viridis', interpolation='nearest')\nplt.colorbar(label='MAE')\nplt.xticks(np.arange(len(learning_rate_values)), learning_rate_values)\nplt.yticks(np.arange(len(n_estimators_values)), n_estimators_values)\nplt.xlabel('Learning Rate')\nplt.ylabel('Number of Estimators')\nplt.title('MAE for different combinations of n_estimators and learning_rate')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:22:34.278183Z","iopub.execute_input":"2023-07-25T21:22:34.278703Z","iopub.status.idle":"2023-07-25T21:22:34.712042Z","shell.execute_reply.started":"2023-07-25T21:22:34.278666Z","shell.execute_reply":"2023-07-25T21:22:34.710453Z"},"trusted":true},"execution_count":13,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m learning_rate_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(key[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 创建一个二维数组，其中包含每个(n_estimators, learning_rate)对应的MAE\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m mae_values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray([[results\u001b[38;5;241m.\u001b[39mget((n_estimators, learning_rate), np\u001b[38;5;241m.\u001b[39mnan) \u001b[38;5;28;01mfor\u001b[39;00m learning_rate \u001b[38;5;129;01min\u001b[39;00m learning_rate_values] \u001b[38;5;28;01mfor\u001b[39;00m n_estimators \u001b[38;5;129;01min\u001b[39;00m n_estimators_values])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# 使用matplotlib创建一个热图\u001b[39;00m\n\u001b[1;32m     33\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"],"ename":"NameError","evalue":"name 'np' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"你可以使用Python的GridSearchCV类来自动测试n_estimators和learning_rate的不同组合。这个类会对每个组合进行交叉验证，然后选择最佳的组合。以下是一个可能的实现：\n\n这将打印出最佳的n_estimators和learning_rate值。注意，由于GridSearchCV使用的是负MAE作为评分，所以最佳的参数是使得负MAE最大（即MAE最小）的参数。","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\n\n# 定义要测试的参数\nparam_grid = {\n    'n_estimators': [100*i for i in range(1,10)],\n    'learning_rate': [0.01*i for i in range(1,10)],\n}\n\n# 创建一个GridSearchCV对象\ngrid_search = GridSearchCV(XGBRegressor(n_jobs=-1), param_grid, cv=5, scoring='neg_mean_absolute_error')\n\n'''\nestimator：这是你想要优化的模型。在你的例子中，这是一个XGBRegressor对象。\n\nparam_grid：这是一个字典或者字典的列表，它定义了要搜索的参数的网格。字典的键是参数的名称，值是尝试的参数值的列表。在你的例子中，你正在尝试四个不同的n_estimators值和五个不同的learning_rate值。\n\nscoring：这是一个字符串，定义了模型评估的方法。在你的例子中，你使用的是负的平均绝对误差（MAE）。这是因为在sklearn中，所有的评分函数都是越高越好，所以损失函数（如MAE）的负值被用作评分函数。\n\ncv：这是一个整数，定义了交叉验证的策略。在你的例子中，你使用的是5折交叉验证，这意味着数据集被分成5个部分，模型在4个部分上进行训练，在剩下的部分上进行评估。这个过程重复5次，每次使用不同的部分进行评估。\n\n'''\n\n# 使用训练数据拟合GridSearchCV对象\ngrid_search.fit(X_train, y_train)\n\n# 打印最佳参数\nprint(grid_search.best_params_)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:27:39.653891Z","iopub.execute_input":"2023-07-25T21:27:39.654325Z","iopub.status.idle":"2023-07-25T21:27:57.430500Z","shell.execute_reply.started":"2023-07-25T21:27:39.654278Z","shell.execute_reply":"2023-07-25T21:27:57.428352Z"},"trusted":true},"execution_count":14,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 25\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03mestimator：这是你想要优化的模型。在你的例子中，这是一个XGBRegressor对象。\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 使用训练数据拟合GridSearchCV对象\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 打印最佳参数\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:1025\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m (\n\u001b[1;32m   1017\u001b[0m     model,\n\u001b[1;32m   1018\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1024\u001b[0m )\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1918\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\n\n# 从grid对象中提取结果，并将其转换为DataFrame\nresults = pd.DataFrame(grid_search.cv_results_)\n\nprint(results[['param_n_estimators', 'param_learning_rate', 'mean_test_score']])\n\n# 使用matplotlib或seaborn等库来绘制结果。例如，你可以创建一个热图来显示不同参数组合的得分：\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npivot = results.pivot('param_n_estimators', 'param_learning_rate', 'mean_test_score')\nsns.heatmap(pivot)\nplt.show()\n\n# 首先，我们需要将结果数据框中的参数列转换为数值类型，以便在热图中使用\nresults['param_n_estimators'] = results['param_n_estimators'].astype(int)\nresults['param_learning_rate'] = results['param_learning_rate'].astype(float)\n\n# 然后，我们创建一个新的数据框，其中的行和列分别对应于不同的参数值，单元格中的值对应于测试得分\npivot = results.pivot('param_n_estimators', 'param_learning_rate', 'mean_test_score')\n\n# 最后，我们使用matplotlib创建热图\nplt.figure(figsize=(10, 8))\nplt.imshow(pivot, cmap='viridis', interpolation='nearest')\nplt.colorbar(label='Mean Test Score')\nplt.xticks(np.arange(len(pivot.columns)), pivot.columns)\nplt.yticks(np.arange(len(pivot.index)), pivot.index)\nplt.xlabel('Learning Rate')\nplt.ylabel('Number of Estimators')\nplt.title('Mean Test Score for different combinations of n_estimators and learning_rate')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:22:34.716160Z","iopub.status.idle":"2023-07-25T21:22:34.717014Z","shell.execute_reply.started":"2023-07-25T21:22:34.716734Z","shell.execute_reply":"2023-07-25T21:22:34.716761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 模型评估方法改为r2","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\n\n# 定义要测试的参数\nparam_grid = {\n    'n_estimators': [100*i for i in range(1,10)],\n    'learning_rate': [0.01*i for i in range(1,10)],\n}\n\n# 创建一个GridSearchCV对象，将评估指标改为'r2'\ngrid_search = GridSearchCV(XGBRegressor(n_jobs=-1), param_grid, cv=5, scoring='r2')\n\n# 使用训练数据拟合GridSearchCV对象\ngrid_search.fit(X_train, y_train)\n\n# 打印最佳参数\nprint(grid_search.best_params_)\n\n# 从grid对象中提取结果，并将其转换为DataFrame\nresults = pd.DataFrame(grid_search.cv_results_)\n\n# 首先，我们需要将结果数据框中的参数列转换为数值类型，以便在热图中使用\nresults['param_n_estimators'] = results['param_n_estimators'].astype(int)\nresults['param_learning_rate'] = results['param_learning_rate'].astype(float)\n\n# 然后，我们创建一个新的数据框，其中的行和列分别对应于不同的参数值，单元格中的值对应于测试得分\npivot = results.pivot('param_n_estimators', 'param_learning_rate', 'mean_test_score')\n\n# 最后，我们使用matplotlib创建热图\nplt.figure(figsize=(10, 8))\nplt.imshow(pivot, cmap='viridis', interpolation='nearest')\nplt.colorbar(label='Mean Test Score')\nplt.xticks(np.arange(len(pivot.columns)), pivot.columns)\nplt.yticks(np.arange(len(pivot.index)), pivot.index)\nplt.xlabel('Learning Rate')\nplt.ylabel('Number of Estimators')\nplt.title('Mean Test Score for different combinations of n_estimators and learning_rate')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:28:16.334913Z","iopub.execute_input":"2023-07-25T21:28:16.335316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# 提取结果\nn_estimators = results['param_n_estimators']\nlearning_rate = results['param_learning_rate']\nscores = results['mean_test_score']\n\n# 创建一个新的图形\nfig, ax = plt.subplots()\n\n# 对于每一个学习率，画一条折线图\nfor lr in learning_rate.unique():\n    # 提取这个学习率的结果\n    mask = learning_rate == lr\n    ax.plot(n_estimators[mask], scores[mask], label=f'Learning Rate: {lr}')\n\n# 添加图例和标签\nax.legend()\nax.set_xlabel('Number of Estimators')\nax.set_ylabel('Mean Test Score')\nax.set_title('Performance of XGBoost with Different Parameters')\n\n# 显示图形\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 改为neg_root_mean_squared_error \n\n>  比赛的评分规则:\n> \n> 提交的评估均方根误差(RMSE)之间的对数的预测值和对数的观察销售价格。(测量原木意味着，预测昂贵房屋和廉价房屋的错误将对结果产生同样的影响。)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\n\n# 定义要测试的参数\nparam_grid = {\n    'n_estimators': [100*i for i in range(1,10)],\n    'learning_rate': [0.01*i for i in range(1,10)],\n}\n\n# 创建一个GridSearchCV对象，将评估指标改为'r2'\ngrid_search = GridSearchCV(XGBRegressor(n_jobs=-1), param_grid, cv=5, scoring='neg_root_mean_squared_error')\n\n# 使用训练数据拟合GridSearchCV对象\ngrid_search.fit(X_train, y_train)\n\n# 打印最佳参数\nprint(grid_search.best_params_)\n\n# 从grid对象中提取结果，并将其转换为DataFrame\nresults = pd.DataFrame(grid_search.cv_results_)\n\n# 首先，我们需要将结果数据框中的参数列转换为数值类型，以便在热图中使用\nresults['param_n_estimators'] = results['param_n_estimators'].astype(int)\nresults['param_learning_rate'] = results['param_learning_rate'].astype(float)\n\n# 然后，我们创建一个新的数据框，其中的行和列分别对应于不同的参数值，单元格中的值对应于测试得分\npivot = results.pivot('param_n_estimators', 'param_learning_rate', 'mean_test_score')\n\n# 最后，我们使用matplotlib创建热图\nplt.figure(figsize=(10, 8))\nplt.imshow(pivot, cmap='viridis', interpolation='nearest')\nplt.colorbar(label='Mean Test Score')\nplt.xticks(np.arange(len(pivot.columns)), pivot.columns)\nplt.yticks(np.arange(len(pivot.index)), pivot.index)\nplt.xlabel('Learning Rate')\nplt.ylabel('Number of Estimators')\nplt.title('Mean Test Score for different combinations of n_estimators and learning_rate')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:22:34.722823Z","iopub.status.idle":"2023-07-25T21:22:34.723753Z","shell.execute_reply.started":"2023-07-25T21:22:34.723515Z","shell.execute_reply":"2023-07-25T21:22:34.723539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"改为随机搜索","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom xgboost import XGBRegressor\n\n# 定义要搜索的参数\nparam_distributions = {\n    \"n_estimators\": [10, 100, 1000, 10000],\n    \"learning_rate\": [0.01, 0.05, 0.1, 0.5, 1]\n}\n\n# 创建模型\nmodel = XGBRegressor(n_jobs=4)\n\n# 创建RandomizedSearchCV对象\nrandom_search = RandomizedSearchCV(\n    estimator=model,\n    param_distributions=param_distributions,\n    n_iter=10,  # 运行的随机搜索迭代次数\n    scoring='neg_mean_absolute_error',  # 评分标准\n    cv=5,  # 交叉验证的折数\n    verbose=2,  # 控制详细程度\n    random_state=42,  # 随机种子，确保结果的可重复性\n    n_jobs=-1,  # 使用所有可用的CPU核心\n    error_score='raise'\n)\n\n# 运行随机搜索\nrandom_search.fit(X, y)\n\n# 打印最佳参数\nprint(random_search.best_params_)\n\nimport pandas as pd\n\n# 从grid对象中提取结果，并将其转换为DataFrame\nresults = pd.DataFrame(random_search.cv_results_)\n\nprint(results[['param_n_estimators', 'param_learning_rate', 'mean_test_score']])\n\n# 使用matplotlib或seaborn等库来绘制结果。例如，你可以创建一个热图来显示不同参数组合的得分：\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npivot = results.pivot('param_n_estimators', 'param_learning_rate', 'mean_test_score')\nsns.heatmap(pivot)\nplt.show()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T21:22:34.724798Z","iopub.status.idle":"2023-07-25T21:22:34.725770Z","shell.execute_reply.started":"2023-07-25T21:22:34.725530Z","shell.execute_reply":"2023-07-25T21:22:34.725554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Keep going\n\nContinue to learn about **[data leakage](https://www.kaggle.com/alexisbcook/data-leakage)**.  This is an important issue for a data scientist to understand, and it has the potential to ruin your models in subtle and dangerous ways!","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/intermediate-machine-learning/discussion) to chat with other learners.*","metadata":{}}]}